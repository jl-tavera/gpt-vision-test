{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping Dog Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Dog Data Scraper\" notebook kicks off our API testing journey by gathering test data. To properly assess our API, we need lots of dog photos. Influencer dogs, known for their high-quality images and identifiable breeds, are perfect for this task. That's why I created a script using selenium to scrape photos of these dogs in Google Images, helping us gather the data we need for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, collecting usernames of dog influencers didn't provide the ideal test sample. Therefore, after researching various websites, I compiled my own list of 50 influencer dogs renowned for their high-quality photos on Google Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs_ig = [\n",
    "    '@itsdougthepug',\n",
    "    '@jiffpom',\n",
    "    '@marniethedog',\n",
    "    '@manny_the_frenchie',\n",
    "    '@crusoe_dachshund',\n",
    "    '@samsonthedood',\n",
    "    '@reagandoodle',\n",
    "    '@barkleysircharles',\n",
    "    '@popeyethefoodie',\n",
    "    '@izzythe.frenchie',\n",
    "    '@tunameltsmyheart',\n",
    "    '@toastmeetsworld',\n",
    "    '@mensweardog',\n",
    "    '@thiswildidea',\n",
    "    '@aspenthemountainpup',\n",
    "    '@dogwithsign',\n",
    "    '@goldenunicornrae',\n",
    "    '@jackson_the_dalmatian',\n",
    "    '@madmax_fluffyroad',\n",
    "    '@pavlovthecorgi',\n",
    "    '@tuckerbudzyn',\n",
    "    '@ppteamkler',\n",
    "    '@Theladyshortcake',\n",
    "    '@rocco_roni',\n",
    "    '@Chompersthecorgi',\n",
    "    '@siberianhusky_jax',\n",
    "    '@good.boy.ollie',\n",
    "    '@bluestaffyboulder',\n",
    "    '@lecorgi',\n",
    "    '@carterchowchow',\n",
    "    '@_gsdbear',\n",
    "    '@harlso',\n",
    "    '@KeyushTheStuntDog',\n",
    "    '@mayapolarbear',\n",
    "    '@marutaro',\n",
    "    '@henrythecoloradodog',\n",
    "    '@eddie_jackrussell',\n",
    "    '@tecuaniventura',\n",
    "    '@ppteamaria',\n",
    "    '@emmatheminifrenchie',\n",
    "    '@balooitsme',\n",
    "    '@pipperontour',\n",
    "    '@mayathedox',\n",
    "    '@hi_im_chewie',\n",
    "    '@frankietothemoon',\n",
    "    '@tinkerbellethedog',\n",
    "    '@loki_the_wolfdog',\n",
    "    '@dailydougie',\n",
    "    '@tikatheiggy',\n",
    "    '@norbertthedog'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To create the scraping function, I followed a tutorial by TechwithTim (https://www.youtube.com/watch?v=NBuED2PivbY&t=1541s). Although the tutorial provided a similar function, I made some modifications to adapt it to our needs. Specifically, I updated it to use classes for finding elements, ensuring it serves our specific purpose effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we developed three primary functions:\n",
    "\n",
    "i) `get_images_from_google(wd, query, delay, max_images)`: This function retrieves the URLs of images in their original scale. It does so by sequentially opening each image that appears in a Google search based on the provided query.\n",
    "\n",
    "ii) `download_image(download_path, url, file_name)`: This function utilizes Pillow to download the image to the local machine.\n",
    "\n",
    "iii) `download_dogs_images(wd, dogs_ig, delay, max_images)`: We also devised a generalized function that iterates through the list of dog influencers and downloads the images one by one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_from_google(wd, query, delay, max_images):\n",
    "\t'''\n",
    "\twd: webdriver\n",
    "\tquery: str\n",
    "\tdelay: int\n",
    "\tmax_images: int\n",
    "\n",
    "\treturns: set\n",
    "\n",
    "\tSave full scale images from google images\n",
    "\t'''\n",
    "\n",
    "\t# Scroll down to load more images\n",
    "\tdef scroll_down(wd):\n",
    "\t\t# Scroll down to the bottom\n",
    "\t\twd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\t\t# Wait to load page\n",
    "\t\ttime.sleep(delay)\n",
    "\t\t\n",
    "\t# Remove the '@' from the query\n",
    "\tquery = query.replace('@', '')\n",
    "\t# Create the url\n",
    "\turl = f\"https://www.google.com/search?q={query}&tbm=isch\"\n",
    "\t# Open the browser\n",
    "\twd.get(url)\n",
    "\t# Create a set to store the image urls\n",
    "\timage_urls = set()\n",
    "\t# Set the number of skips to 0\n",
    "\tskips = 0\n",
    "\t# Scroll down to load more images\n",
    "\twhile len(image_urls) + skips < max_images:\n",
    "\t\t# Scroll down\n",
    "\t\tscroll_down(wd)\n",
    "\t\t# Find the thumbnails\n",
    "\t\tthumbnails = wd.find_elements(By.CLASS_NAME, \"Q4LuWd\")\n",
    "\n",
    "\t\t# Click on the thumbnails to get the full scale images\n",
    "\t\tfor img in thumbnails[len(image_urls) + skips:max_images]:\n",
    "\t\t\ttry:\n",
    "\t\t\t\timg.click()\n",
    "\t\t\t\ttime.sleep(delay)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Find the full scale images\n",
    "\t\t\timages = wd.find_elements(By.CLASS_NAME, \"sFlh5c\")\n",
    "\t\t\t# Add the image to the set\n",
    "\t\t\tfor image in images:\n",
    "\t\t\t\t# If the image is already in the set, skip it\n",
    "\t\t\t\tif image.get_attribute('src') in image_urls:\n",
    "\t\t\t\t\t# Add to the skips\n",
    "\t\t\t\t\tmax_images += 1\n",
    "\t\t\t\t\tskips += 1\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t# If the image is a full scale image, add it to the set\n",
    "\t\t\t\tif image.get_attribute('src') and 'http' in image.get_attribute('src'):\n",
    "\t\t\t\t\timage_urls.add(image.get_attribute('src'))\n",
    "\t\t\t\t\tprint(f\"Found {len(image_urls)}\")\n",
    "\n",
    "\treturn image_urls\n",
    "\n",
    "\n",
    "def download_image(download_path, url, file_name):\n",
    "\t'''\n",
    "\tdownload_path: str\n",
    "\turl: str\n",
    "\tfile_name: str\n",
    "\n",
    "\treturns: None\n",
    "\n",
    "\tDownload an image from a url\n",
    "\t'''\n",
    "\t\n",
    "\ttry:\n",
    "\t\t# Create the download path if it doesn't exist\n",
    "\t\timage_content = requests.get(url).content\n",
    "\t\t# Open the image\n",
    "\t\timage_file = io.BytesIO(image_content)\n",
    "\t\timage = Image.open(image_file)\n",
    "\t\t# Save the image\n",
    "\t\tfile_path = download_path + file_name\n",
    "\t\twith open(file_path, \"wb\") as f:\n",
    "\t\t\timage.save(f, \"JPEG\")\n",
    "\t\tprint(\"Success\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint('FAILED -', e)\n",
    "\n",
    "\n",
    "def download_dogs_images(wd, dogs_ig, delay, max_images):\n",
    "\t'''\n",
    "\twd: webdriver\n",
    "\tdogs_ig: list\n",
    "\tdelay: int\n",
    "\tmax_images: int\n",
    "\n",
    "\treturns: None\n",
    "\n",
    "\tDownload images of dogs from google images\n",
    "\t'''\n",
    "\n",
    "\t# Iterate over the dogs\n",
    "\tfor dog in dogs_ig:\n",
    "\t\t# Set the query to the dog's name\n",
    "\t\tquery = dog\n",
    "\t\t# Get the image urls\n",
    "\t\turls = get_images_from_google(wd, query, delay, max_images)\n",
    "\t\t# Download the images\n",
    "\t\tfor i, url in enumerate(urls):\n",
    "\t\t\tdownload_image(\"imgs/\", url, str(dog) + str(i) + \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to organize image data efficiently, I scripted a process in Python. Initially, I specified the folder path containing the images and listed all files within it. Then, I iterated through each image file, extracting its dimensions, file size, and aspect ratio. Using Pandas, I structured this data into a DataFrame, which I sorted and exported to a CSV file named \"image_data.csv\" for easy reference and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing images\n",
    "folder_path = \"imgs/\"\n",
    "\n",
    "# List all files in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Initialize lists to store file names and dimensions\n",
    "file_names = []\n",
    "dimensions = []\n",
    "file_sizes = []\n",
    "aspect_ratios = []\n",
    "\n",
    "# Iterate through each file\n",
    "for file in files:\n",
    "    # Check if the file is an image\n",
    "    if file.endswith((\".png\", \".jpg\", \".jpeg\", \".gif\")):\n",
    "        # Get the full path of the image\n",
    "        image_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Open the image\n",
    "        with Image.open(image_path) as img:\n",
    "            # Get the dimensions\n",
    "            width, height = img.size\n",
    "            # Calculate aspect ratio\n",
    "            aspect_ratio = width / height\n",
    "            # Get file size\n",
    "            file_size = os.path.getsize(image_path) / (1024 * 1024)  # Convert to MB\n",
    "            # Append file name, dimensions, file size, and aspect ratio to the lists\n",
    "            file_names.append(file)\n",
    "            dimensions.append((width, height))\n",
    "            file_sizes.append(file_size)\n",
    "            aspect_ratios.append(aspect_ratio)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\"File Name\": file_names, \n",
    "                   \"Dimensions\": dimensions, \n",
    "                   \"File Size (MB)\": file_sizes,\n",
    "                   \"Aspect Ratio\": aspect_ratios})\n",
    "df = df.sort_values(by=\"File Name\")\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv(\"./output/raw/images/image_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
